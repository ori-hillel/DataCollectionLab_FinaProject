{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b648756b-6c05-4b97-b9e1-f1770803af9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Smart Renovations for Maximum Airbnb Revenue\n",
    "\n",
    "### Budget-Constrained Optimization to Identify Renovations That Maximize Nightly Price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca5438-26fb-407d-8122-4b5fc9245f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435eccf7-e8c8-4a06-a764-1a86bcd7c204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_state = 'New York'            #fill your state with its full name, with capital letters (i.e. 'New York')\n",
    "\n",
    "\n",
    "user_ratings = 4.55               #fill the average rating of your airbnb\n",
    "user_rating_cleanliness = 4.6     #fill the average cleanliness rating of your airbnb\n",
    "user_rating_accuracy = 4.7        #fill the average accuracy rating of your airbnb\n",
    "user_rating_check_in = 4.9        #fill the average check in rating of your airbnb\n",
    "user_rating_communication = 4.9   #fill the average communication rating of your airbnb\n",
    "user_rating_location = 4.8        #fill the average location  rating of your airbnb\n",
    "user_rating_value = 4.7           #fill the average value rating of your airbnb\n",
    "user_guests = 4                   #fill the amount of guests allowed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#In the following fields fill as so:\n",
    "#fill with 0 if you don't have this amenity  \n",
    "#fill with 1 if you do have this amenity\n",
    "#fill with 2 if you don't have this amenity but you can't have it (i.e a playground in an apartment) or \n",
    "# if you don't want to have it (i.e don't want to deal with a pool)\n",
    "\n",
    "user_air_conditioning = 0   # Air conditioning\n",
    "user_bbq_grill = 0          # BBQ grill\n",
    "user_basement = 0           # BasemenAt\n",
    "user_coffee_maker = 1       # Coffee maker\n",
    "user_crib = 0               # Crib\n",
    "user_ev_charger = 0         # EV charger\n",
    "user_elevator = 1           # Elevator\n",
    "user_fan = 0                # Fan\n",
    "user_gaming = 0             # Gaming console\n",
    "user_gym = 0                # Gym\n",
    "user_heating = 1            # Heating\n",
    "user_hot_tub_sauna = 0      # Hot tub/sauna\n",
    "user_microwave = 0          # Microwave\n",
    "user_outdoor_space = 1      # Outdoor space\n",
    "user_parking = 1            # Parking\n",
    "user_pool = 1               # Pool\n",
    "user_refrigerator = 1       # Refrigerator\n",
    "user_safety_equipment = 1   # Safety equipment\n",
    "user_scenic_view = 1        # Scenic view\n",
    "user_sound_system = 0       # Sound system\n",
    "user_stove_oven = 1         # Stove/oven\n",
    "user_tv = 0                 # TV\n",
    "user_washer_dryer = 1       # Washer/dryer\n",
    "user_wifi_internet = 1      # WiFi/internet\n",
    "user_workspace = 0          # Workspace (desk & chair)\n",
    "user_bikes = 0              # Bikes\n",
    "user_playground = 0         # Playground\n",
    "\n",
    "\n",
    "\n",
    "# In the following field, add your budget in USD\n",
    "user_budget = 5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d50acf0-6c46-43e3-947c-ba0f69604389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## HIDDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882143f9-0a0c-497e-a2d5-d92c6f9a6e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading the prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92982c75-73de-4779-86d3-ae0fb81ddb08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "file_path_base_cost_map = \"Aviv_Oded_Ori/base_cost_map.csv\"\n",
    "\n",
    "sas_token_data = \"sp=rle&st=2026-01-25T10:55:58Z&se=2026-03-01T19:10:58Z&spr=https&sv=2024-11-04&sr=c&sig=jgt2r2TSHpDaCyEfTEgHAfkvEvy49xReFDS4Mg9KnOA%3D\"\n",
    "storage_account = \"lab94290\"\n",
    "container_submissions = \"submissions\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token_data)\n",
    "\n",
    "csv_url = f\"abfss://{container_submissions}@{storage_account}.dfs.core.windows.net/{file_path_base_cost_map}\"\n",
    "\n",
    "base_cost_map_df = spark.read.option(\"header\", \"true\").csv(csv_url)\n",
    "\n",
    "\n",
    "# 4. Convert to local dictionary: key='item', value='base_cost' (as float)\n",
    "base_cost_map = {row['item']: float(row['base_cost']) for row in base_cost_map_df.select(\"item\", \"base_cost\").collect()}\n",
    "# print(base_cost_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc347a5-6a86-44b8-b73b-3218a0bc439c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading the state factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204274c3-8c62-4d3e-82cb-beca8f7ab3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sas_token_data = \"sp=rle&st=2026-01-25T10:55:58Z&se=2026-03-01T19:10:58Z&spr=https&sv=2024-11-04&sr=c&sig=jgt2r2TSHpDaCyEfTEgHAfkvEvy49xReFDS4Mg9KnOA%3D\"\n",
    "storage_account = \"lab94290\"\n",
    "container_submissions = \"submissions\"\n",
    "file_path_state_factors = \"Aviv_Oded_Ori/state_factors.csv\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token_data)\n",
    "\n",
    "csv_url = f\"abfss://{container_submissions}@{storage_account}.dfs.core.windows.net/{file_path_state_factors}\"\n",
    "\n",
    "state_factors_df = spark.read.option(\"header\", \"true\").csv(csv_url)\n",
    "# display(state_factors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399f97ba-7f31-4cee-8d02-2591cae8e931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Multiplying the prices by the state factor (according to the user's input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca9630a9-5545-4645-a694-2c7ce7584180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "factor = float(state_factors_df.filter(f\"State_Full = '{user_state}'\").select('COL_Factor').collect()[0][0])\n",
    "amenity_costs = {k: float(v) * factor for k, v in base_cost_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3b88fc-88ab-4b7d-a139-f649ca95886f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loading the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9ff56b-4d6e-4674-8dd8-9191bd681253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Attempting Direct HTTP Download...\n   URL: https://lab94290.blob.core.windows.net/submissions/Aviv_Oded_Ori/airbnb_rf_model_v1.zip?[HIDDEN_TOKEN]\n   ✅ Connection established. Downloading bytes...\n   ✅ Download complete.\n2. Unzipping to /tmp/airbnb_rf_model_v1...\n3. Moving model to DBFS (dbfs:/FileStore/models/airbnb_rf_model_v1)...\n4. Loading model into Spark...\n✅ Success! Model loaded into variable 'loaded_model'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "storage_account = \"lab94290\"\n",
    "container = \"submissions\"\n",
    "folder_path = \"Aviv_Oded_Ori\"\n",
    "zip_filename = \"airbnb_rf_model_v1.zip\"\n",
    "\n",
    "# Your SAS Token (Exactly as you provided)\n",
    "sas_token = \"sp=rle&st=2026-01-25T10:55:58Z&se=2026-03-01T19:10:58Z&spr=https&sv=2024-11-04&sr=c&sig=jgt2r2TSHpDaCyEfTEgHAfkvEvy49xReFDS4Mg9KnOA%3D\"\n",
    "\n",
    "# --- 2. DIRECT HTTP DOWNLOAD ---\n",
    "# We use 'blob.core.windows.net' to treat this as a simple web download\n",
    "# This avoids all Spark/Hadoop permission complexities.\n",
    "download_url = f\"https://{storage_account}.blob.core.windows.net/{container}/{folder_path}/{zip_filename}?{sas_token}\"\n",
    "\n",
    "local_zip_path = f\"/tmp/{zip_filename}\"\n",
    "local_unzip_dir = f\"/tmp/{zip_filename.replace('.zip', '')}\"\n",
    "final_dbfs_path = f\"dbfs:/FileStore/models/{zip_filename.replace('.zip', '')}\"\n",
    "\n",
    "print(f\"1. Attempting Direct HTTP Download...\")\n",
    "print(f\"   URL: https://{storage_account}.blob.core.windows.net/{container}/{folder_path}/{zip_filename}?[HIDDEN_TOKEN]\")\n",
    "\n",
    "# Request the file\n",
    "response = requests.get(download_url, stream=True)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"   ✅ Connection established. Downloading bytes...\")\n",
    "    with open(local_zip_path, 'wb') as f:\n",
    "        shutil.copyfileobj(response.raw, f)\n",
    "    print(\"   ✅ Download complete.\")\n",
    "\n",
    "    # --- 3. UNZIP AND MOVE ---\n",
    "    print(f\"2. Unzipping to {local_unzip_dir}...\")\n",
    "    shutil.unpack_archive(local_zip_path, local_unzip_dir)\n",
    "\n",
    "    print(f\"3. Moving model to DBFS ({final_dbfs_path})...\")\n",
    "    dbutils.fs.rm(final_dbfs_path, recurse=True) # Clean up old versions\n",
    "    dbutils.fs.cp(f\"file:{local_unzip_dir}\", final_dbfs_path, recurse=True)\n",
    "\n",
    "    # --- 4. LOAD MODEL ---\n",
    "    print(\"4. Loading model into Spark...\")\n",
    "    try:\n",
    "        loaded_model = PipelineModel.load(final_dbfs_path)\n",
    "        loaded = True\n",
    "        print(\"✅ Success! Model loaded into variable 'loaded_model'\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Download worked, but Spark could not load the model folder: {e}\")\n",
    "        loaded = False\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Download Failed. HTTP Status Code: {response.status_code}\")\n",
    "    print(f\"   Reason: {response.text}\")\n",
    "    loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3144c1a5-7231-46f1-947a-0efa5316c769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "base_row_dict = {\n",
    "    'state': user_state,\n",
    "    'guests': user_guests,\n",
    "    'Air Conditioning': user_air_conditioning,\n",
    "    'BBQ Grill': user_bbq_grill,\n",
    "    'Basement': user_basement,\n",
    "    'Coffee Maker': user_coffee_maker,\n",
    "    'Crib': user_crib,\n",
    "    'EV Charger': user_ev_charger,\n",
    "    'Elevator': user_elevator,\n",
    "    'Fan': user_fan,\n",
    "    'Gaming': user_gaming,\n",
    "    'Gym': user_gym,\n",
    "    'Heating': user_heating,\n",
    "    'Hot Tub/Sauna': user_hot_tub_sauna,\n",
    "    'Microwave': user_microwave,\n",
    "    'Outdoor Space': user_outdoor_space,\n",
    "    'Parking': user_parking,\n",
    "    'Pool': user_pool,\n",
    "    'Refrigerator': user_refrigerator,\n",
    "    'Safety Equipment': user_safety_equipment,\n",
    "    'Scenic View': user_scenic_view,\n",
    "    'Sound System': user_sound_system,\n",
    "    'Stove/Oven': user_stove_oven,\n",
    "    'TV': user_tv,\n",
    "    'Washer/Dryer': user_washer_dryer,\n",
    "    'Wifi/Internet': user_wifi_internet,\n",
    "    'Workspace': user_workspace,\n",
    "    'bikes': user_bikes,\n",
    "    'playground': user_playground,\n",
    "    'rating_Cleanliness': user_rating_cleanliness,\n",
    "    'rating_Accuracy': user_rating_accuracy,\n",
    "    'rating_Check_in': user_rating_check_in,\n",
    "    'rating_Communication': user_rating_communication,\n",
    "    'rating_Location': user_rating_location,\n",
    "    'rating_Value': user_rating_value,\n",
    "    'rating_clean': user_ratings\n",
    "}\n",
    "\n",
    "not_check = ['state', 'guests', 'rating_Cleanliness', 'rating_Accuracy', 'rating_Check_in', 'rating_Communication', 'rating_Location', 'rating_Value', 'rating_clean']\n",
    "\n",
    "forbidden = []\n",
    "for k,v in base_row_dict.items():\n",
    "    if k != 'state':\n",
    "        base_row_dict[k] = float(v)\n",
    "    if k not in not_check and v == 2:\n",
    "        v = 0\n",
    "        base_row_dict[k] = 0\n",
    "        forbidden.append(k)\n",
    "        \n",
    "print(forbidden)\n",
    "\n",
    "base_row_df = spark.createDataFrame([base_row_dict])\n",
    "# display(base_row_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23990099-c40a-4961-9038-4f8a9f5bb002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Hill Climb + Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9783a4c-c0a5-4660-afb8-3abedab92fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "\n",
    "def optimize_amenities_beam(base_row_df, model, budget, amenity_costs, forbidden_amenities=None, B=3):\n",
    "    \"\"\"\n",
    "    Optimizes rental property amenities using Beam Search with a Spark Model,\n",
    "    respecting a list of forbidden upgrades.\n",
    "    \n",
    "    Parameters:\n",
    "    - spark: Active SparkSession\n",
    "    - base_row_df: A 1-row Spark DataFrame representing the property's current state.\n",
    "    - model: Trained Spark ML PipelineModel.\n",
    "    - budget: Float, maximum amount to spend.\n",
    "    - amenity_costs: Dict { 'AmenityName': Price(float) }.\n",
    "    - forbidden_amenities: List[str] of amenities that cannot be installed.\n",
    "    - B: Int, Beam Width (hyperparameter).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. SETUP & INITIAL SCAN ---\n",
    "    base_row = base_row_df.collect()[0]\n",
    "    base_features = base_row.asDict()\n",
    "    \n",
    "    # Handle the forbidden list (convert to set for faster lookup)\n",
    "    forbidden_set = set(forbidden_amenities) if forbidden_amenities else set()\n",
    "    \n",
    "    # Get Baseline Price\n",
    "    base_pred_log = model.transform(base_row_df).select(\"prediction\").collect()[0][0]\n",
    "    base_pred = math.expm1(base_pred_log)\n",
    "    \n",
    "    print(f\"--- Starting Optimization (Beam Width={B}) ---\")\n",
    "    print(f\"Initial Price: ${base_pred:.2f} | Budget: ${budget}\")\n",
    "    if forbidden_set:\n",
    "        print(f\"Excluding: {forbidden_set}\")\n",
    "\n",
    "    # Define the universe of valid upgrades:\n",
    "    # 1. Must be in our cost dictionary\n",
    "    # 2. Must NOT be in the forbidden list (NEW CHECK)\n",
    "    # 3. Must NOT already exist in the property (value == 0)\n",
    "    # 4. Must be affordable (Cost <= Budget)\n",
    "    possible_upgrades = [\n",
    "        am for am in amenity_costs.keys()\n",
    "        if am in base_features          # Ensure column exists in data\n",
    "        and am not in forbidden_set     # <--- NEW CONSTRAINT\n",
    "        and base_features[am] == 0      # User doesn't have it yet\n",
    "        and amenity_costs[am] <= budget # Affordable\n",
    "    ]\n",
    "    \n",
    "    print(f\"Identified {len(possible_upgrades)} valid potential upgrades.\")\n",
    "\n",
    "    # The \"Beam\" (List of candidate objects)\n",
    "    current_beam = [{\n",
    "        \"added_amenities\": set(),\n",
    "        \"cost\": 0.0,\n",
    "        \"price\": base_pred,\n",
    "        \"features\": base_features\n",
    "    }]\n",
    "\n",
    "    best_solution = current_beam[0]\n",
    "    step = 0\n",
    "    \n",
    "    while True:\n",
    "        step += 1\n",
    "        next_step_candidates = {} # Deduplication dict (Key: frozenset)\n",
    "        \n",
    "        batch_rows_data = []\n",
    "        batch_metadata = [] \n",
    "\n",
    "        # --- 2. EXPANSION ---\n",
    "        for parent_candidate in current_beam:\n",
    "            \n",
    "            # Find amenities we can add to THIS path\n",
    "            valid_moves = [\n",
    "                am for am in possible_upgrades\n",
    "                if am not in parent_candidate[\"added_amenities\"] \n",
    "                and (parent_candidate[\"cost\"] + amenity_costs[am]) <= budget\n",
    "            ]\n",
    "            \n",
    "            for move in valid_moves:\n",
    "                # Create State Key (Deduplication)\n",
    "                new_set = parent_candidate[\"added_amenities\"].union({move})\n",
    "                state_key = frozenset(new_set)\n",
    "                \n",
    "                if state_key in next_step_candidates:\n",
    "                    continue\n",
    "                \n",
    "                # Construct new features\n",
    "                new_features = parent_candidate[\"features\"].copy()\n",
    "                new_features[move] = 1.0\n",
    "                new_features[\"_temp_id\"] = len(batch_rows_data)\n",
    "                \n",
    "                batch_rows_data.append(new_features)\n",
    "                batch_metadata.append({\n",
    "                    \"id\": new_features[\"_temp_id\"],\n",
    "                    \"amenities\": new_set,\n",
    "                    \"cost\": parent_candidate[\"cost\"] + amenity_costs[move],\n",
    "                    \"features\": new_features\n",
    "                })\n",
    "                \n",
    "                next_step_candidates[state_key] = None \n",
    "\n",
    "        # --- 3. BATCH PREDICTION ---\n",
    "        if not batch_rows_data:\n",
    "            print(\"No further affordable improvements found.\")\n",
    "            break\n",
    "            \n",
    "        print(f\"Step {step}: Evaluating {len(batch_rows_data)} unique combinations...\")\n",
    "        \n",
    "        batch_df = spark.createDataFrame(batch_rows_data)\n",
    "        predictions = model.transform(batch_df).select(\"_temp_id\", \"prediction\").collect()\n",
    "        pred_map = {row[\"_temp_id\"]: row[\"prediction\"] for row in predictions}\n",
    "\n",
    "        # --- 4. SELECTION ---\n",
    "        valid_candidates_list = []\n",
    "        \n",
    "        for meta in batch_metadata:\n",
    "            pred_log = pred_map.get(meta[\"id\"])\n",
    "            if pred_log is None: continue \n",
    "\n",
    "            pred_price = math.expm1(pred_log)\n",
    "\n",
    "            # Hill Climb Check: Only keep if price improves\n",
    "            if pred_price > best_solution[\"price\"]: \n",
    "                candidate_obj = {\n",
    "                    \"added_amenities\": meta[\"amenities\"],\n",
    "                    \"cost\": meta[\"cost\"],\n",
    "                    \"price\": pred_price,\n",
    "                    \"features\": meta[\"features\"]\n",
    "                }\n",
    "                valid_candidates_list.append(candidate_obj)\n",
    "                \n",
    "                if pred_price > best_solution[\"price\"]:\n",
    "                    best_solution = candidate_obj\n",
    "\n",
    "        # --- 5. PRUNING ---\n",
    "        if not valid_candidates_list:\n",
    "            print(\"No moves improved the price. Stopping.\")\n",
    "            break\n",
    "            \n",
    "        valid_candidates_list.sort(key=lambda x: x[\"price\"], reverse=True)\n",
    "        current_beam = valid_candidates_list[:B]\n",
    "        \n",
    "        print(f\"   -> Top Choice: {list(current_beam[0]['added_amenities'])} \"\n",
    "              f\"(Price: ${current_beam[0]['price']:.2f})\")\n",
    "\n",
    "    # --- 6. FINAL OUTPUT ---\n",
    "    return {\n",
    "        \"final_amenities\": list(best_solution[\"added_amenities\"]),\n",
    "        \"original_price\": base_pred,\n",
    "        \"final_predicted_price\": best_solution[\"price\"],\n",
    "        \"price_lift\": best_solution[\"price\"] - base_pred,\n",
    "        \"total_cost\": best_solution[\"cost\"],\n",
    "        \"roi_ratio\": (best_solution[\"price\"] - base_pred) / best_solution[\"cost\"] if best_solution[\"cost\"] > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f73f2fe-26f9-4e72-aedd-ac14973088cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# --- GENERATE UI ---\n",
    "def render_optimization_card(res):\n",
    "    # Formatting\n",
    "    roi_pct = res['roi_ratio'] * 100\n",
    "    lift = res['price_lift']\n",
    "    cost = res['total_cost']\n",
    "    orig = res['original_price']\n",
    "    new_p = res['final_predicted_price']\n",
    "    \n",
    "    # Generate Amenity Badges\n",
    "    badges_html = \"\"\n",
    "    for item in res['final_amenities']:\n",
    "        badges_html += f\"\"\"\n",
    "            <span style=\"background-color: #e3f2fd; color: #1565c0; \n",
    "                         padding: 5px 12px; border-radius: 15px; \n",
    "                         font-size: 14px; font-weight: 500; margin-right: 5px; \n",
    "                         display: inline-block; margin-bottom: 5px;\">\n",
    "                + {item}\n",
    "            </span>\n",
    "        \"\"\"\n",
    "\n",
    "    # HTML Template\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;\n",
    "                max-width: 700px; background: white; border-radius: 12px; \n",
    "                box-shadow: 0 4px 20px rgba(0,0,0,0.08); border: 1px solid #e0e0e0; overflow: hidden; margin: 10px 0;\">\n",
    "        \n",
    "        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; color: white;\">\n",
    "            <h2 style=\"margin: 0; font-size: 20px; font-weight: 600;\">✨ Optimization Complete</h2>\n",
    "            <p style=\"margin: 5px 0 0 0; opacity: 0.9; font-size: 14px;\">Best upgrade path found for your budget</p>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px; padding: 25px; border-bottom: 1px solid #f0f0f0;\">\n",
    "            <div style=\"text-align: center;\">\n",
    "                <div style=\"font-size: 12px; color: #666; text-transform: uppercase; letter-spacing: 1px; font-weight: bold;\">ROI Ratio</div>\n",
    "                <div style=\"font-size: 28px; font-weight: 700; color: #2e7d32; margin-top: 5px;\">{roi_pct:.3f}%</div>\n",
    "                <div style=\"font-size: 12px; color: #2e7d32;\">Return Efficiency</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; border-left: 1px solid #eee; border-right: 1px solid #eee;\">\n",
    "                <div style=\"font-size: 12px; color: #666; text-transform: uppercase; letter-spacing: 1px; font-weight: bold;\">Price Lift</div>\n",
    "                <div style=\"font-size: 28px; font-weight: 700; color: #1565c0; margin-top: 5px;\">+${lift:.2f}</div>\n",
    "                <div style=\"font-size: 12px; color: #666;\">Per Night</div>\n",
    "            </div>\n",
    "            <div style=\"text-align: center;\">\n",
    "                <div style=\"font-size: 12px; color: #666; text-transform: uppercase; letter-spacing: 1px; font-weight: bold;\">Total Cost</div>\n",
    "                <div style=\"font-size: 28px; font-weight: 700; color: #424242; margin-top: 5px;\">${cost:,.0f}</div>\n",
    "                <div style=\"font-size: 12px; color: #666;\">Investment</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"padding: 25px;\">\n",
    "            <div style=\"margin-bottom: 20px;\">\n",
    "                <div style=\"font-size: 14px; color: #444; font-weight: 600; margin-bottom: 10px;\">Recommended Installations:</div>\n",
    "                <div>{badges_html}</div>\n",
    "            </div>\n",
    "            \n",
    "            <div style=\"background: #fafafa; padding: 15px; border-radius: 8px; display: flex; justify-content: space-between; align-items: center;\">\n",
    "                <div>\n",
    "                    <span style=\"color: #666; font-size: 14px;\">Original Price:</span>\n",
    "                    <strong style=\"color: #333; margin-left: 5px;\">${orig:.2f}</strong>\n",
    "                </div>\n",
    "                <div style=\"color: #999;\">➜</div>\n",
    "                <div>\n",
    "                    <span style=\"color: #666; font-size: 14px;\">New Price:</span>\n",
    "                    <strong style=\"color: #333; margin-left: 5px;\">${new_p:.2f}</strong>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    displayHTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d8b61c8-5da3-41cb-9eeb-34a67e3c6e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Optimization (Beam Width=20) ---\nInitial Price: $163.01 | Budget: $5000\nIdentified 11 valid potential upgrades.\nStep 1: Evaluating 11 unique combinations...\n   -> Top Choice: ['Sound System'] (Price: $167.85)\nStep 2: Evaluating 10 unique combinations...\n   -> Top Choice: ['Sound System', 'BBQ Grill'] (Price: $177.04)\nStep 3: Evaluating 6 unique combinations...\n   -> Top Choice: ['Sound System', 'Crib', 'BBQ Grill'] (Price: $179.70)\nStep 4: Evaluating 6 unique combinations...\n   -> Top Choice: ['Sound System', 'Crib', 'Microwave', 'BBQ Grill'] (Price: $181.59)\nNo further affordable improvements found.\n"
     ]
    }
   ],
   "source": [
    "    output = optimize_amenities_beam(base_row_df, loaded_model, user_budget, amenity_costs, forbidden, B=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea972d9-d535-4352-8029-c38482f3f2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac09bfa-f1ad-4651-89e0-efd4537a55ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;\n",
       "                max-width: 700px; background: white; border-radius: 12px; \n",
       "                box-shadow: 0 4px 20px rgba(0,0,0,0.08); border: 1px solid #e0e0e0; overflow: hidden; margin: 10px 0;\">\n",
       "        \n",
       "        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; color: white;\">\n",
       "            <h2 style=\"margin: 0; font-size: 20px; font-weight: 600;\">✨ Optimization Complete</h2>\n",
       "            <p style=\"margin: 5px 0 0 0; opacity: 0.9; font-size: 14px;\">Best upgrade path found for your budget</p>\n",
       "        </div>\n",
       "\n",
       "        <div style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px; padding: 25px; border-bottom: 1px solid #f0f0f0;\">\n",
       "            <div style=\"text-align: center;\">\n",
       "                <div style=\"font-size: 12px; color: #666; text-transform: uppercase; letter-spacing: 1px; font-weight: bold;\">ROI Ratio</div>\n",
       "                <div style=\"font-size: 28px; font-weight: 700; color: #2e7d32; margin-top: 5px;\">0.376%</div>\n",
       "                <div style=\"font-size: 12px; color: #2e7d32;\">Return Efficiency</div>\n",
       "            </div>\n",
       "            <div style=\"text-align: center; border-left: 1px solid #eee; border-right: 1px solid #eee;\">\n",
       "                <div style=\"font-size: 12px; color: #666; text-transform: uppercase; letter-spacing: 1px; font-weight: bold;\">Price Lift</div>\n",
       "                <div style=\"font-size: 28px; font-weight: 700; color: #1565c0; margin-top: 5px;\">+$18.58</div>\n",
       "                <div style=\"font-size: 12px; color: #666;\">Per Night</div>\n",
       "            </div>\n",
       "            <div style=\"text-align: center;\">\n",
       "                <div style=\"font-size: 12px; color: #666; text-transform: uppercase; letter-spacing: 1px; font-weight: bold;\">Total Cost</div>\n",
       "                <div style=\"font-size: 28px; font-weight: 700; color: #424242; margin-top: 5px;\">$4,947</div>\n",
       "                <div style=\"font-size: 12px; color: #666;\">Investment</div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div style=\"padding: 25px;\">\n",
       "            <div style=\"margin-bottom: 20px;\">\n",
       "                <div style=\"font-size: 14px; color: #444; font-weight: 600; margin-bottom: 10px;\">Recommended Installations:</div>\n",
       "                <div>\n",
       "            <span style=\"background-color: #e3f2fd; color: #1565c0; \n",
       "                         padding: 5px 12px; border-radius: 15px; \n",
       "                         font-size: 14px; font-weight: 500; margin-right: 5px; \n",
       "                         display: inline-block; margin-bottom: 5px;\">\n",
       "                + Sound System\n",
       "            </span>\n",
       "        \n",
       "            <span style=\"background-color: #e3f2fd; color: #1565c0; \n",
       "                         padding: 5px 12px; border-radius: 15px; \n",
       "                         font-size: 14px; font-weight: 500; margin-right: 5px; \n",
       "                         display: inline-block; margin-bottom: 5px;\">\n",
       "                + Crib\n",
       "            </span>\n",
       "        \n",
       "            <span style=\"background-color: #e3f2fd; color: #1565c0; \n",
       "                         padding: 5px 12px; border-radius: 15px; \n",
       "                         font-size: 14px; font-weight: 500; margin-right: 5px; \n",
       "                         display: inline-block; margin-bottom: 5px;\">\n",
       "                + Microwave\n",
       "            </span>\n",
       "        \n",
       "            <span style=\"background-color: #e3f2fd; color: #1565c0; \n",
       "                         padding: 5px 12px; border-radius: 15px; \n",
       "                         font-size: 14px; font-weight: 500; margin-right: 5px; \n",
       "                         display: inline-block; margin-bottom: 5px;\">\n",
       "                + BBQ Grill\n",
       "            </span>\n",
       "        </div>\n",
       "            </div>\n",
       "            \n",
       "            <div style=\"background: #fafafa; padding: 15px; border-radius: 8px; display: flex; justify-content: space-between; align-items: center;\">\n",
       "                <div>\n",
       "                    <span style=\"color: #666; font-size: 14px;\">Original Price:</span>\n",
       "                    <strong style=\"color: #333; margin-left: 5px;\">$163.01</strong>\n",
       "                </div>\n",
       "                <div style=\"color: #999;\">➜</div>\n",
       "                <div>\n",
       "                    <span style=\"color: #666; font-size: 14px;\">New Price:</span>\n",
       "                    <strong style=\"color: #333; margin-left: 5px;\">$181.59</strong>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if loaded:\n",
    "    render_optimization_card(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b03238-27c6-4c8d-834f-1b86c0216671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db03926f-48e2-45f3-aeda-3fdbb7a1dfa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if loaded:\n",
    "    dbutils.notebook.exit(\"Stopping execution here\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9c7bbe-4fd0-4410-82ea-8b04b4097201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Reading and preprocessing the data from `Airbnb.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4577c1-c987-467c-b2a4-08a9a62c4d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"lab94290\"  \n",
    "container = \"airbnb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "641dbe21-45e3-498e-9bf6-962fcec97402",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 7"
    }
   },
   "outputs": [],
   "source": [
    "sas_token=\"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\"\n",
    "sas_token = sas_token.lstrip('?')\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a568a2e-9acd-4984-b2f9-620c86189306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/airbnb_1_12_parquet\"\n",
    "\n",
    "airbnb = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb1a74b-54d4-4768-944e-fb65cdaa8ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "airbnb = (\n",
    "    airbnb\n",
    "    .withColumn(\"arr\", F.split(F.col(\"location\"), \",\"))   # split into array\n",
    "    .withColumn(\"country\", F.element_at(F.col(\"arr\"), -1))\n",
    "    .withColumn(\"state\", F.element_at(F.col(\"arr\"), -2))\n",
    "    .drop(\"arr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a698119-f1c7-4f24-8bdc-6084eee0b2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are only interested in countries from the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42351b02-09ea-42e6-8bd6-ab696acf64ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airbnb_usa = airbnb.filter(\"country == ' United States'\")\n",
    "airbnb_usa = airbnb_usa.select(\"name\", \"price\", \"ratings\", \"location\", \"lat\", \"long\", \"state\", \"amenities\", 'guests','category_rating')\n",
    "display(airbnb_usa.limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a497bebd-2441-46a5-ba43-ad5b0ee714f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extracting the state from the coordinates, as is much more accurate and reliable than using the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2ba3db-90e5-4b95-af7f-d3948df5b497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install reverse_geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d0d787-f414-4f95-8d30-f231d135f419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import reverse_geocoder as rg\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "# --- 1. DEFINE SCHEMA FOR EFFICIENT RETURN ---\n",
    "# We define a structure so the function can return two values at once\n",
    "location_schema = StructType([\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"city_state\", StringType(), True)\n",
    "])\n",
    "\n",
    "# --- 2. DEFINE PYTHON FUNCTION ---\n",
    "def get_geo_data_pure(lat, lon):\n",
    "    # Handle Nulls\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # mode=1 gives the single nearest result\n",
    "        results = rg.search((lat, lon), mode=1)\n",
    "        \n",
    "        # Extract components\n",
    "        city = results[0]['name']\n",
    "        state = results[0]['admin1']\n",
    "        \n",
    "        # Return tuple matching the schema: (state, city_state)\n",
    "        return (state, f\"{city}, {state}\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- 3. REGISTER UDF ---\n",
    "get_geo_udf = udf(get_geo_data_pure, location_schema)\n",
    "\n",
    "# --- 4. APPLY TO DATAFRAME ---\n",
    "# Step A: Cast coordinates to double\n",
    "df_processed = airbnb_usa.withColumn(\"lat\", col(\"lat\").cast(\"double\")) \\\n",
    "                         .withColumn(\"long\", col(\"long\").cast(\"double\"))\n",
    "\n",
    "# Step B: Call UDF once to create a temporary struct column\n",
    "df_processed = df_processed.withColumn(\"geo_temp\", get_geo_udf(col(\"lat\"), col(\"long\")))\n",
    "\n",
    "# Step C: Unpack the struct into your two desired columns\n",
    "airbnb_usa = df_processed.withColumn(\"state\", col(\"geo_temp.state\")) \\\n",
    "                               .withColumn(\"city_state\", col(\"geo_temp.city_state\")) \\\n",
    "                               .drop(\"geo_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89c1c9e4-148a-46ef-904a-e9eb017241fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "states_list = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "airbnb_usa = airbnb_usa.filter(col(\"state\").isin(states_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd192e9-fa65-4f5d-b966-34c93cdc5931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading the scraped data from `bestplaces.net`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae532485-cbcb-4d74-8777-f619b1776e27",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 17"
    }
   },
   "outputs": [],
   "source": [
    "sas_token_data = \"sp=rle&st=2026-01-25T10:55:58Z&se=2026-03-01T19:10:58Z&spr=https&sv=2024-11-04&sr=c&sig=jgt2r2TSHpDaCyEfTEgHAfkvEvy49xReFDS4Mg9KnOA%3D\"\n",
    "storage_account = \"lab94290\"\n",
    "container_submissions = \"submissions\"\n",
    "file_path_state_factors = \"Aviv_Oded_Ori/state_factors.csv\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token_data)\n",
    "\n",
    "csv_url = f\"abfss://{container_submissions}@{storage_account}.dfs.core.windows.net/{file_path_state_factors}\"\n",
    "\n",
    "state_factors_df = spark.read.option(\"header\", \"true\").csv(csv_url)\n",
    "# display(state_factors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0afa4e99-3add-45b0-a698-0853300538c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extracting amenities and categorizing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a5634ce-0cc9-4644-98fc-efb0cca5a170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, explode\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "\n",
    "# 1. DEFINE THE SCHEMA\n",
    "json_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"group_name\", StringType(), True),\n",
    "        StructField(\"items\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"value\", StringType(), True)\n",
    "            ])\n",
    "        ), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 2. OPTIMIZATION: Get only unique amenity strings first!\n",
    "unique_amenities_df = airbnb_usa.select(\"amenities\").dropDuplicates()\n",
    "\n",
    "# 3. PARSE AND EXPLODE\n",
    "distinct_amenities_df = (unique_amenities_df\n",
    "    # Parse the JSON\n",
    "    .withColumn(\"parsed_amenities\", from_json(col(\"amenities\"), json_schema))\n",
    "    # Explode Groups (e.g., \"Bathroom\", \"Kitchen\")\n",
    "    .withColumn(\"group\", explode(col(\"parsed_amenities\")))\n",
    "    # Explode Items (e.g., \"Shampoo\", \"Oven\")\n",
    "    .withColumn(\"item\", explode(col(\"group.items\")))\n",
    "    # Select just the name\n",
    "    .select(col(\"item.name\").alias(\"amenity_name\"))\n",
    "    # Final cleanup to remove duplicates in the resulting names\n",
    "    .distinct()\n",
    "    .orderBy(\"amenity_name\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d0dfa0-0853-4e4f-a93b-252be360d276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4685ee-f953-4356-acdd-bf49a868b29d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 21"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lower\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 1. Define the Python Logic (No Pandas)\n",
    "def map_amenity_logic(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "        \n",
    "    # Normalize\n",
    "    text = str(text).lower().strip()\n",
    "    \n",
    "    # --- EXCLUSION LIST (Simple Amenities) ---\n",
    "    # These return None so we can filter them out later\n",
    "    excludes = [\n",
    "        'shampoo', 'conditioner', 'soap', 'body wash', 'shower gel', \n",
    "        'toilet paper', 'towel', 'linen', 'pillow', 'blanket', 'sheet', \n",
    "        'hanger', 'trash', 'paper towel', 'salt', 'pepper', 'oil', 'spice',\n",
    "        'toothbrush', 'toothpaste', 'cotton', 'tissue', 'toiletries', 'boooks', 'toys', 'ski', 'housekeeping', 'butler'\n",
    "    ]\n",
    "    if any(x in text for x in excludes):\n",
    "        return None \n",
    "\n",
    "    # --- MAPPING RULES ---\n",
    "    \n",
    "    # 1. Luxury / Water\n",
    "    if 'pool' in text: return 'Pool'\n",
    "    if any(x in text for x in ['hot tub', 'jacuzzi', 'sauna', 'steam room']): return 'Hot Tub/Sauna'\n",
    "    \n",
    "    # 2. Tech & Entertainment\n",
    "    if any(x in text for x in ['tv', 'television', 'hdtv', 'netflix', 'roku', 'hbo', 'apple tv', 'cable', '4k']): return 'TV'\n",
    "    if any(x in text for x in ['wifi','wi-fi', 'internet', 'ethernet', 'broadband']): return 'Wifi/Internet'\n",
    "    if any(x in text for x in ['sound system', 'speaker', 'bluetooth', 'sound bar', 'sonos']): return 'Sound System'\n",
    "    if any(x in text for x in ['game', 'console', 'ps4', 'xbox', 'nintendo', 'arcade']): return 'Gaming'\n",
    "    \n",
    "    # 3. Major Appliances\n",
    "    if any(x in text for x in ['washer', 'dryer', 'laundry', 'washing machine']): return 'Washer/Dryer'\n",
    "    if any(x in text for x in ['dishwasher']): return 'Dishwasher'\n",
    "    if any(x in text for x in ['fridge', 'refrigerator', 'freezer']): return 'Refrigerator'\n",
    "    if any(x in text for x in ['stove', 'oven', 'cooktop', 'range', 'cooker', 'hot plate']): return 'Stove/Oven'\n",
    "    if any(x in text for x in ['Rice cooker', 'Rice maker', 'rice cooker', 'Rice maker']): return 'Rice Cooker'\n",
    "\n",
    "    if any(x in text for x in ['microwave']): return 'Microwave'\n",
    "    if any(x in text for x in ['coffee', 'espresso', 'keurig', 'nespresso']): return 'Coffee Maker'\n",
    "    if any(x in text for x in ['park', 'parking']): return 'Parking'\n",
    "    if any(x in text for x in ['cellar']): return 'Basement'\n",
    "\n",
    "    \n",
    "    # 4. Climate Control\n",
    "    if any(x in text for x in ['air condition', 'a/c', 'ac unit', 'hvac', 'mini split', ' ac ']): return 'Air Conditioning'\n",
    "    if any(x in text for x in ['heat', 'fireplace', 'wood stove', 'pellet stove']): return 'Heating'\n",
    "    if any(x in text for x in ['fan', 'ceiling fan']): return 'Fan'\n",
    "    \n",
    "    # 5. Facilities\n",
    "    if any(x in text for x in ['gym', 'fitness', 'exercise', 'weight', 'treadmill', 'yoga', 'peloton']): return 'Gym'\n",
    "    if any(x in text for x in ['parking', 'garage', 'carport', 'driveway']): return 'Parking'\n",
    "    if any(x in text for x in ['elevator', 'lift']): return 'Elevator'\n",
    "    if any(x in text for x in ['desk', 'monitor', 'office', 'workspace']): return 'Workspace'\n",
    "    \n",
    "    # 6. Outdoor\n",
    "    if any(x in text for x in ['bbq', 'grill', 'barbecue']): return 'BBQ Grill'\n",
    "    if any(x in text for x in ['patio', 'balcony', 'terrace', 'deck', 'backyard', 'garden']): return 'Outdoor Space'\n",
    "    if any(x in text for x in ['view', 'ocean', 'lake', 'mountain', 'waterfront']): return 'Scenic View'\n",
    "\n",
    "    # 7. Safety (Optional - keep if you want to track safety features)\n",
    "    if any(x in text for x in ['detector', 'alarm', 'extinguisher', 'aid kit']): return 'Safety Equipment'\n",
    "    if any(x in text for x in ['EV charger', 'EV-charger', 'ev charger']): return 'EV Charger'\n",
    "    if any(x in text for x in ['crib', 'Crib','paid crib']): return 'Crib'\n",
    "    if any(x in text for x in ['bike', 'bikes','Bike', 'Bikes']): return 'bikes'\n",
    "    if any(x in text for x in ['playground', 'Playground']): return 'playground'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return \"Other\"\n",
    "\n",
    "# 2. Register the UDF\n",
    "# We specify StringType() as the return type\n",
    "clean_amenity_udf = udf(map_amenity_logic, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1e192b-7729-4028-ac48-774f05dee842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sas_token=\"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\"\n",
    "sas_token = sas_token.lstrip('?')\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34644489-cd5a-44d6-99f8-9d1093ce14a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"id\" not in airbnb_usa.columns:\n",
    "    df_main = airbnb_usa.withColumn(\"listing_id_temp\", F.monotonically_increasing_id())\n",
    "    id_col = \"listing_id_temp\"\n",
    "else:\n",
    "    df_main = airbnb_usa\n",
    "    id_col = \"id\"\n",
    "\n",
    "# A. Parse JSON & Explode\n",
    "# 1. Parse string -> Array of Groups\n",
    "df_parsed = df_main.withColumn(\"parsed_amenities\", F.from_json(F.col(\"amenities\"), json_schema))\n",
    "\n",
    "# 2. Explode Nested Structure (Groups -> Items)\n",
    "df_exploded = df_parsed \\\n",
    "    .select(id_col, F.explode(\"parsed_amenities\").alias(\"group\")) \\\n",
    "    .select(id_col, F.explode(\"group.items\").alias(\"item\")) \\\n",
    "    .select(id_col, F.col(\"item.name\").alias(\"raw_amenity_name\"))\n",
    "\n",
    "# B. Apply Categorization\n",
    "df_mapped = df_exploded.withColumn(\"clean_category\", clean_amenity_udf(F.col(\"raw_amenity_name\")))\n",
    "\n",
    "# C. Filter (Remove Nulls/Simples) & Pivot\n",
    "# We pivot on 'clean_category' to create columns like 'TV', 'Pool'\n",
    "df_features = df_mapped \\\n",
    "    .filter(F.col(\"clean_category\").isNotNull()) \\\n",
    "    .groupBy(id_col) \\\n",
    "    .pivot(\"clean_category\") \\\n",
    "    .agg(F.lit(1)) \\\n",
    "    .na.fill(0)\n",
    "\n",
    "# --- 4. JOIN BACK (Optional) ---\n",
    "# Join the feature vectors back to the original dataframe\n",
    "# Using left join to keep all original rows, filling missing features with 0\n",
    "final_df = df_main.join(df_features, on=id_col, how=\"left\").na.fill(0)\n",
    "\n",
    "# Display\n",
    "# display(final_df.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1371a668-5f70-43ab-8f6c-fe9b839acc03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = final_df.drop(\"Other\")\n",
    "# display(final_df.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189bb114-9525-4bae-8ad0-21de0c2e45fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = final_df.dropna(subset=['guests'])\n",
    "final_df = final_df.withColumn(\"guests\", col(\"guests\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032fb7f8-fda2-4977-977e-d52940a3e42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, map_from_arrays\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "\n",
    "# 1. DEFINE SCHEMA\n",
    "rating_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 2. PARSE AND EXTRACT (Updating final_df directly)\n",
    "# First, create the map helper columns\n",
    "final_df = final_df.withColumn(\"parsed_ratings\", from_json(col(\"category_rating\"), rating_schema)) \\\n",
    "                   .withColumn(\"ratings_map\", map_from_arrays(col(\"parsed_ratings.name\"), col(\"parsed_ratings.value\")))\n",
    "\n",
    "# 3. CREATE INDIVIDUAL COLUMNS\n",
    "target_ratings = [\"Cleanliness\", \"Accuracy\", \"Check-in\", \"Communication\", \"Location\", \"Value\"]\n",
    "new_rating_cols = []\n",
    "\n",
    "for rating_name in target_ratings:\n",
    "    # Define the new column name (e.g., rating_Cleanliness)\n",
    "    col_name = f\"rating_{rating_name.replace('-', '_')}\"\n",
    "    new_rating_cols.append(col_name)\n",
    "    \n",
    "    # Add it to final_df\n",
    "    final_df = final_df.withColumn(\n",
    "        col_name, \n",
    "        col(\"ratings_map\").getItem(rating_name).cast(\"float\")\n",
    "    )\n",
    "\n",
    "# 4. CLEAN UP\n",
    "# Fill missing values with 0 and remove the temporary helper columns\n",
    "final_df = final_df.fillna(0, subset=new_rating_cols) \\\n",
    "                   .drop(\"parsed_ratings\", \"ratings_map\")\n",
    "\n",
    "# 5. VERIFY\n",
    "print(\"Columns added successfully!\")\n",
    "# display(final_df.limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503435f3-1a77-4bcd-b9f7-38e0a2e8a3e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training The Model (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81533c4e-22c6-40a1-8e4c-055342108fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import col, log1p, expm1\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import os\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"log_label\",   # Train on LOG price\n",
    "    numTrees=100,           \n",
    "    maxDepth=12,            # 12 is a sweet spot for speed vs accuracy\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "loaded = False\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "model_path = \"/Workspace/Users/orihillel@campus.technion.ac.il/94290_Databricks/databricks-course/notebooks/airbnb_price_model_v1\"\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        model = PipelineModel.load(model_path)\n",
    "        loaded = True\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model loading failed: {e}\")\n",
    "else:\n",
    "    print(\"Model path does not exist. Model not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cac7d9c-8762-4f2c-b89f-58b27bdbd9d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check the data type and sample values from final_df\n",
    "final_df_2 = final_df.limit(1000).filter(col('price') > 0)\n",
    "price_data = [float(row['price']) for row in final_df_2.select(\"price\").collect()]\n",
    "\n",
    "# Calculate upper fence\n",
    "q1 = np.percentile(price_data, 25)\n",
    "q3 = np.percentile(price_data, 75)\n",
    "iqr = q3 - q1\n",
    "upper_fence = q3 + 1.5 * iqr\n",
    "print(\"Upper fence value:\", upper_fence)\n",
    "\n",
    "plt.boxplot(price_data, vert=True)\n",
    "plt.title(\"Price Distribution\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.axhline(upper_fence, color='red', linestyle='--', label=f'Upper Fence: {upper_fence:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf39871c-5b1f-4b08-bd3f-8b2a759815f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- FIX 1: FILTER OUTLIERS ---\n",
    "# Keep only listings > $0 and <= $2000\n",
    "filtered_df = final_df.filter((col(\"price\") > 0) & (col(\"price\") <= 2000))\n",
    "\n",
    "# Prepare Data\n",
    "model_df = filtered_df \\\n",
    "    .withColumn(\"label\", col(\"price\").cast(\"float\")) \\\n",
    "    .withColumn(\"log_label\", log1p(col(\"price\").cast(\"float\"))) \\\n",
    "    .withColumn(\"rating_clean\", col(\"ratings\").cast(\"float\")) \\\n",
    "    .fillna(0, subset=[\"rating_clean\"]) \\\n",
    "    .dropna(subset=[\"label\", \"log_label\"])\n",
    "\n",
    "train_data, test_data = model_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# --- FIX 2: LOCATION (STATE ONLY) ---\n",
    "# We removed City to make this run fast (minutes instead of hours)\n",
    "state_indexer = StringIndexer(inputCol=\"State\", outputCol=\"State_Index\", handleInvalid=\"keep\")\n",
    "state_encoder = OneHotEncoder(inputCols=[\"State_Index\"], outputCols=[\"State_Vec\"])\n",
    "\n",
    "# Define inputs\n",
    "amenities = [\n",
    "    'Air Conditioning', 'BBQ Grill', 'Basement', 'Coffee Maker', 'Crib', \n",
    "    'EV Charger', 'Elevator', 'Fan', 'Gaming', 'Gym', 'Heating', \n",
    "    'Hot Tub/Sauna', 'Microwave', 'Outdoor Space', 'Parking', 'Pool', \n",
    "    'Refrigerator', 'Safety Equipment', 'Scenic View', 'Sound System', \n",
    "    'Stove/Oven', 'TV', 'Washer/Dryer', 'Wifi/Internet', 'Workspace', \n",
    "    'bikes', 'playground', 'guests', \n",
    "    'rating_Cleanliness', 'rating_Accuracy', 'rating_Check_in', \n",
    "    'rating_Communication', 'rating_Location', 'rating_Value'\n",
    "]\n",
    "\n",
    "# Add 'State_Vec' (City is gone)\n",
    "all_inputs = amenities + ['State_Vec', 'rating_clean']\n",
    "all_inputs = [c for c in all_inputs if c and c.strip() != \"\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=all_inputs, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# --- FIX 3: INCREASE MODEL POWER ---\n",
    "\n",
    "# Pipeline (City stages removed)\n",
    "pipeline = Pipeline(stages=[state_indexer, state_encoder, assembler, rf])\n",
    "\n",
    "print(\"Training Optimized Model (State Only)...\")\n",
    "model = pipeline.fit(train_data)\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# --- EVALUATE (Convert back to Dollars) ---\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Convert log predictions back to real dollars to calculate RMSE\n",
    "predictions = predictions.withColumn(\"prediction_dollars\", expm1(\"prediction\"))\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction_dollars\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction_dollars\", metricName=\"r2\")\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"New RMSE: ${rmse:.2f}\")\n",
    "print(f\"R-Squared: {r2:.2%}\")\n",
    "display(predictions.select(\"label\", \"prediction_dollars\", \"State\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7592f3-1140-4c47-8360-8a272a6ea550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Choose a unique path so it doesn't clash with others\n",
    "# 'dbfs:/' is the protocol Spark uses to talk to the file system\n",
    "SAVE_PATH = \"dbfs:/FileStore/models/airbnb_rf_model_v1\"\n",
    "\n",
    "print(f\"Saving model to: {SAVE_PATH} ...\")\n",
    "\n",
    "# --- Saving ---\n",
    "# 'model' is the variable name of your trained model (e.g., from pipeline.fit or crossval.fit)\n",
    "model.write().overwrite().save(SAVE_PATH)\n",
    "\n",
    "print(\"✅ Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d3cb17-3814-48f6-a5dc-40bcc50a715c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "\n",
    "# --- Configuration ---\n",
    "# Must match the path used above\n",
    "LOAD_PATH = \"dbfs:/FileStore/models/airbnb_rf_model_v1\"\n",
    "\n",
    "print(f\"Attempting to load model from: {LOAD_PATH} ...\")\n",
    "\n",
    "try:\n",
    "    # --- Loading ---\n",
    "    # option A: Use PipelineModel if you used pipeline.fit()\n",
    "    loaded_model = PipelineModel.load(LOAD_PATH)\n",
    "    \n",
    "    # option B: Use CrossValidatorModel if you used crossval.fit()\n",
    "    # loaded_model = CrossValidatorModel.load(LOAD_PATH)\n",
    "\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    print(f\"Model type: {type(loaded_model)}\")\n",
    "    \n",
    "    # Optional: Print a stage to prove it's real\n",
    "    # print(loaded_model.stages[-1]) \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f03e7e2-79c3-423b-bb91-e166a7eccf95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading the scraped data from `costco.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca9be4f-7e32-4e44-a107-60aa6a212fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path_costco = \"Aviv_Oded_Ori/costco.csv\"\n",
    "\n",
    "sas_token_data = \"sp=rle&st=2026-01-25T10:55:58Z&se=2026-03-01T19:10:58Z&spr=https&sv=2024-11-04&sr=c&sig=jgt2r2TSHpDaCyEfTEgHAfkvEvy49xReFDS4Mg9KnOA%3D\"\n",
    "storage_account = \"lab94290\"\n",
    "container_submissions = \"submissions\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token_data)\n",
    "csv_url = f\"abfss://{container_submissions}@{storage_account}.dfs.core.windows.net/{file_path_costco}\"\n",
    "\n",
    "costco_df = spark.read.option(\"header\", \"true\").csv(csv_url)\n",
    "# display(costco_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e9ab4f-8030-4b11-a898-586d155390f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sas_token=\"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\"\n",
    "sas_token = sas_token.lstrip('?')\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ae6463-62cd-4e03-938c-3cdd4d0e72c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#taking the median of the costco prices\n",
    "from pyspark.sql.functions import col, expr\n",
    "median_costs_df = costco_df.groupBy(\"Keyword\") \\\n",
    "    .agg(\n",
    "        expr(\"percentile_approx(Price, 0.5)\").alias(\"Median_Cost\")\n",
    "    )\n",
    "\n",
    "# 3. View the results\n",
    "# display(median_costs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72cd4aab-4611-498a-bd26-008a77dfe9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hill climbing + Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76f6ae0e-eb20-439d-8509-db4c2a95f522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loading the aggregated prices from websites such as Costco, Fixr, Target, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d589999-ae48-4d3c-a0a2-c1f6a70622b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "file_path_base_cost_map = \"Aviv_Oded_Ori/base_cost_map.csv\"\n",
    "\n",
    "sas_token_data = \"sp=rle&st=2026-01-25T10:55:58Z&se=2026-03-01T19:10:58Z&spr=https&sv=2024-11-04&sr=c&sig=jgt2r2TSHpDaCyEfTEgHAfkvEvy49xReFDS4Mg9KnOA%3D\"\n",
    "storage_account = \"lab94290\"\n",
    "container_submissions = \"submissions\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token_data)\n",
    "\n",
    "csv_url = f\"abfss://{container_submissions}@{storage_account}.dfs.core.windows.net/{file_path_base_cost_map}\"\n",
    "\n",
    "base_cost_map_df = spark.read.option(\"header\", \"true\").csv(csv_url)\n",
    "\n",
    "\n",
    "# 4. Convert to local dictionary: key='item', value='base_cost' (as float)\n",
    "base_cost_map = {row['item']: float(row['base_cost']) for row in base_cost_map_df.select(\"item\", \"base_cost\").collect()}\n",
    "# print(base_cost_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a553d8d-d8a7-40cf-8319-6f21b2e7e873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "factor = float(state_factors_df.filter(f\"State_Full = '{user_state}'\").select('COL_Factor').collect()[0][0])\n",
    "amenity_costs = {k: float(v) * factor for k, v in base_cost_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42807fd8-2d46-4279-8c30-963b98ea4088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sas_token=\"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\"\n",
    "sas_token = sas_token.lstrip('?')\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9cfeefd-e8f6-4b0c-a9a1-e76fcfc5e0b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read the input data, and create the row which will be inserted into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f1e0274-a6bd-4575-aeb9-e33b52f9f000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_row_dict = {\n",
    "    'state': user_state,\n",
    "    'guests': user_guests,\n",
    "    'Air Conditioning': user_air_conditioning,\n",
    "    'BBQ Grill': user_bbq_grill,\n",
    "    'Basement': user_basement,\n",
    "    'Coffee Maker': user_coffee_maker,\n",
    "    'Crib': user_crib,\n",
    "    'EV Charger': user_ev_charger,\n",
    "    'Elevator': user_elevator,\n",
    "    'Fan': user_fan,\n",
    "    'Gaming': user_gaming,\n",
    "    'Gym': user_gym,\n",
    "    'Heating': user_heating,\n",
    "    'Hot Tub/Sauna': user_hot_tub_sauna,\n",
    "    'Microwave': user_microwave,\n",
    "    'Outdoor Space': user_outdoor_space,\n",
    "    'Parking': user_parking,\n",
    "    'Pool': user_pool,\n",
    "    'Refrigerator': user_refrigerator,\n",
    "    'Safety Equipment': user_safety_equipment,\n",
    "    'Scenic View': user_scenic_view,\n",
    "    'Sound System': user_sound_system,\n",
    "    'Stove/Oven': user_stove_oven,\n",
    "    'TV': user_tv,\n",
    "    'Washer/Dryer': user_washer_dryer,\n",
    "    'Wifi/Internet': user_wifi_internet,\n",
    "    'Workspace': user_workspace,\n",
    "    'bikes': user_bikes,\n",
    "    'playground': user_playground,\n",
    "    'rating_Cleanliness': user_rating_cleanliness,\n",
    "    'rating_Accuracy': user_rating_accuracy,\n",
    "    'rating_Check_in': user_rating_check_in,\n",
    "    'rating_Communication': user_rating_communication,\n",
    "    'rating_Location': user_rating_location,\n",
    "    'rating_Value': user_rating_value,\n",
    "    'rating_clean': user_ratings\n",
    "}\n",
    "\n",
    "not_check = ['state', 'guests', 'rating_Cleanliness', 'rating_Accuracy', 'rating_Check_in', 'rating_Communication', 'rating_Location', 'rating_Value', 'rating_clean']\n",
    "\n",
    "forbidden = []\n",
    "for k,v in base_row_dict.items():\n",
    "    if k != 'state':\n",
    "        base_row_dict[k] = float(v)\n",
    "    if k not in not_check and v == 2:\n",
    "        v = 0\n",
    "        base_row_dict[k] = 0\n",
    "        forbidden.append(k)\n",
    "        \n",
    "print(forbidden)\n",
    "\n",
    "base_row_df = spark.createDataFrame([base_row_dict])\n",
    "# display(base_row_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c16bb50-2290-4200-b1a5-3123ebf17236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f249c42a-f064-402c-ae08-6d7ec5020f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output = optimize_amenities_beam(base_row_df, model, user_budget, amenity_costs, forbidden, B=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bdccc2b-b10f-4256-87d7-d83ef0ad8cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "render_optimization_card(output)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7263496967750255,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ROI_Optimizer",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}